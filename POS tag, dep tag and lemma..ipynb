{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a82ac33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.1 Create a Doc object from the file owlcreek.txt\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "with open(\"owlcreek.txt\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1e2ef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4835\n"
     ]
    }
   ],
   "source": [
    "#Question.2 How many tokens are contained in the file?\n",
    "num_tokens = len([token for token in doc])\n",
    "print(f\"Number of tokens: {num_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb6ec30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 204\n"
     ]
    }
   ],
   "source": [
    "#Question.3 How many sentences are contained in the file?\n",
    "num_sentences = len(list(doc.sents))\n",
    "print(f\"Number of sentences: {num_sentences}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ef53209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence: The man's hands were behind\n",
      "his back, the wrists bound with a cord.  \n"
     ]
    }
   ],
   "source": [
    "#Question.4 Print the second sentence in the document\n",
    "sentences = list(doc.sents)\n",
    "second_sentence = sentences[1]\n",
    "print(f\"Second sentence: {second_sentence.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd2f9623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in second sentence with POS, dep, and lemma:\n",
      "('The', 'DET', 'det', 'the')\n",
      "('man', 'NOUN', 'poss', 'man')\n",
      "(\"'s\", 'PART', 'case', \"'s\")\n",
      "('hands', 'NOUN', 'nsubj', 'hand')\n",
      "('were', 'AUX', 'ROOT', 'be')\n",
      "('behind', 'ADP', 'prep', 'behind')\n",
      "('\\n', 'SPACE', 'dep', '\\n')\n",
      "('his', 'PRON', 'poss', 'his')\n",
      "('back', 'NOUN', 'pobj', 'back')\n",
      "(',', 'PUNCT', 'punct', ',')\n",
      "('the', 'DET', 'det', 'the')\n",
      "('wrists', 'NOUN', 'appos', 'wrist')\n",
      "('bound', 'VERB', 'acl', 'bind')\n",
      "('with', 'ADP', 'prep', 'with')\n",
      "('a', 'DET', 'det', 'a')\n",
      "('cord', 'NOUN', 'pobj', 'cord')\n",
      "('.', 'PUNCT', 'punct', '.')\n",
      "(' ', 'SPACE', 'dep', ' ')\n"
     ]
    }
   ],
   "source": [
    "#Question.5 For each token in the sentence above, print its text, POS tag, dep tag and lemma.\n",
    "second_sentence_token_info = [(token.text, token.pos_, token.dep_, token.lemma_) for token in second_sentence]\n",
    "print(\"Tokens in second sentence with POS, dep, and lemma:\")\n",
    "for token_info in second_sentence_token_info:\n",
    "    print(token_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef0fde54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.6 Write a matcher called 'Swimming' that finds both occurrences of the phrase \"swimming vigorously\" in the text.\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\": \"swim\"}, {\"LEMMA\": \"vigorously\"}]\n",
    "matcher.add(\"Swimming\", [pattern])\n",
    "matches = matcher(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9020c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text surrounding each 'swimming vigorously' match:\n"
     ]
    }
   ],
   "source": [
    "#Question.7 Print the text surrounding each found match.\n",
    "surrounding_text = []\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start-5:end+5]  # 5 tokens before and after\n",
    "    surrounding_text.append(span.text)\n",
    "\n",
    "print(\"Text surrounding each 'swimming vigorously' match:\")\n",
    "for text in surrounding_text:\n",
    "    print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb32ccdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6a9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
